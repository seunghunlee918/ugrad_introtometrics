\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{framed}
\usepackage{tikz}
%\usepackage{charter}
\usepackage{mathpazo}
%\usepackage{newcent}
\usepackage{indentfirst}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{mdframed}
\usetikzlibrary{trees}
\pagestyle{fancy}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{property}
\newtheorem{property}{Property}[section]
\theoremstyle{assumption}
\newtheorem{assumption}{Assumption}[section]
\theoremstyle{example}
\newtheorem{example}{Example}[section]
\theoremstyle{comment}
\newtheorem{comment}{Comment}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{lastpage}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{setspace}
\hypersetup{
colorlinks=true,
linkcolor=black,
filecolor=green, 
urlcolor=blue,
}
\newcommand{\ROM}[1]
    {\MakeUppercase{\romannumeral #1}}
\fancyhead[L]{Econometrics \ROM{2}: Recitation 12 }%change each reci
\fancyhead[R]{Spring 2020}
\fancyfoot[C]{\thepage \hspace{1pt} / \pageref{LastPage}}

\fancypagestyle{firstpage}{%
\fancyhf{}%
\renewcommand{\headrulewidth}{0mm}%
  \fancyfoot[C]{\thepage \hspace{1pt} / \pageref{LastPage}}
}

\usepackage{wrapfig}

\lhead{Introduction to Econometrics}

\rhead{Recitation 7}


\title{Introduction to Econometrics: Recitation 7}

\begin{document}
\linespread{1.25}
\author{Seung-hun Lee}
\date{}
\maketitle
%%%%%%%%%%%%%%%%%%
\section{Panel Regression}
\subsection{Motivation for Using Panel Data}
We now get to analyze a different type of dataset. We are moving into the \textbf{panel data}, where we observe multiple individuals for multiple periods of time. In terms of notation, we write
\[
Y_{it} = \beta_0 + \beta_1X_{1,it}+ ... +\beta_kX_{k,it}+u_{it}
\]
where $i=1,2,...,N$ indicates individuals and $t=1,2,...,T$ indicates time periods. For some panel datasets, there are $T$ datasets for each of the $N$ individuals included in the data. This type of panel data is said to be a \textbf{balanced panel data}. However, most panel data available to public has cases where there are $t\leq T$ datapoints for some of the $N$ individuals. This type of dataset is an \textbf{unbalanced panel data}. \par\medskip
There are many reasons for using a panel data. One is simply that using a panel data allows us to use more datasets. As we have learned from the previous lectures, our estimates get easier and more accurate as we have more data. However, a crucial advantage of using a panel dataset comes from the fact that it can allow us to control for a particular type of omitted variable bias. Specifically, it can allow us to control for \textbf{unobserved heterogeneity} that are either 1) different accross $N$ entities but always remain same for $T$ periods in a given state or 2) different accross $T$ times periods but remains the same for all $N$ entities in a particular time period or 3) both of 1) and 2). The first of this is called a \textbf{cross section fixed effect}. The second one is \textbf{time fixed effects}. Case 3) is what we call \textbf{two-way fixed effects}.\par\medskip
\begin{mdframed}[backgroundcolor =blue!10]
\textit{For curious minds}: Can the unobservable heterogeneity be unrelated to the independent variables? In theory, yes. This type of unobserved heterogeneity is commonly referred to as \textbf{random effects}. In reality, not many data satisfy this. So we will delay the discussion of this topic to an advanced course 
\end{mdframed} \par\medskip
To see why that is the case, suppose that $T=2$ and we are interested in the relationship between vehicle related fatality rate (deaths per 10,000 people) and the beer tax. Suppose that we get these result for the two years
\[
\begin{aligned}
\hat{Y}_{i1} &=2.01 &+ 0.15X_{i1}\\
                    &(0.15)&(0.20) \\
\hat{Y}_{i2} &=1.86 &+ 0.44X_{i2}\\
                    &(0.11)&(0.20) \\                    
\end{aligned}
\]
In the first year, the coefficient on $X_i$ is not significant at all, whereas the same coefficient for year 2 is very significant. In such case, one might suspect that there is an omitted variable bias that affects these coefficients. For instance, if $i$ denotes states, one might guess that some type of omitted variable that are specific to the states could be affecting these results - like strictness of DUI laws in some state $i$. If it is the case that the strictness of DUI laws did not change for the two time periods, then these effects qualify as a state fixed effect (replace cross section with state). \par\medskip 
To see how such omitted variable bias can be controlled for, let $Z_i$ denote the strictness of state laws on DUI. This is not exactly measurable. However, if there is no significant change in state law for the two periods, $Z_i$ can be considered unchanging. Now write
\[
\begin{aligned}
Y_{i1}& = \beta_0 + \beta_1X_{i1}+\beta_2 Z_{i}+u_{i1} \\                
Y_{i2}& = \beta_0 + \beta_1X_{i2}+\beta_2 Z_{i}+u_{i2} \\                
\end{aligned}
\]
Subtract the second equation from the first to get
\[
(Y_{i2}-Y_{i1}) = \beta_1(X_{i2}-X_{i1}) +\beta_2(Z_{i}-Z_{i}) + u_{i2}-u_{i1}
\]
With $Z_i$ being the same for all periods, the above equation is reduced to
\[
(Y_{i2}-Y_{i1}) = \beta_1(X_{i2}-X_{i1}) +(u_{i2}-u_{i1})
\]
The $Z_i$ variable has no role in this equation. Effectively, we did control for state fixed effect that are constant across time. If we estimate this particular $\beta_1$, we can obtain much more accurate estimates of the effect of bear tax on fatality rate. The $\beta_1$ in this case can be interpreted as how much the change in ($X_{i2}-X_{i1}$) changes ($Y_{i2}-Y_{i1}$). \par\medskip
Let's use a numerical example, Suppose that beer tax is \$10 in year 1, so $X_{i1}=10$. Suppose that state $i$ raises its bear tax by $1$ dollars, therefore $X_{i2}-X_{i1}=1$. If $\beta_1$ is estimated as  $-1.1$, then we can write the predicted change in $Y$ (which would be a predicted change in death per 10,000 people) as
\[
\widehat{Y_{i2}-Y_{i1}} = -1.1(X_{i2}-X_{i1})
\]
If you put  $X_{i2}-X_{i1}=1$, Then $\widehat{Y_{i2}-Y_{i1}} = -1.1$, or that fatality per 10,000 reduces by 1.1. Also notice that this is starkly different result compared to the estimates we got when we regressed year by year (coefficient in front of $X$ was positive, as opposed to a negative coefficient for change in $X$ we used for the second approach). \par\medskip
\subsection{Methodology}
We are going to limit our discussion to a cross-sectional fixed effects, as all logic would hold by changing cross sectional index $i$ to time index $t$. For each of these fixed effects, there are two ways of estimating the data when $T\geq 3$. One of them is to include $N-1$ individual dummy variables (or $T-1$ time period dummy variables), sometimes referred to as \textbf{least square dummy variable method}. The other is to subtract from the original equation the ``demeaned" equation, which is referred to as \textbf{within estimation}. \par\medskip
For both approaches, the following framework will work. Assume that the true relation between $Y$ and $X$ variable is 
\[
Y_{it}=\beta_0 + \beta_1X_{it}+\beta_2 Z_{i}+u_{it} \tag{1}
\]
where $Z_i$ is the cross section fixed effect. For $i=1,...,N$, the value of $\beta_2 Z_i$ is different. If we were to express this equation on an $(X,Y)$-plane, we would end up with a different intercept for each $i$. So for simplification, define $\alpha_i = \beta_0 + \beta_1Z_i$. Then the above equation can be written as
\[
Y_{it}=\beta_1X_{it}+\alpha_{i}+u_{it} \tag{2}
\]
in equation (2), the $\alpha_i$ term can be thought of as an effect of being an entity $i$. Since this is correlated with $X_{it}$, we deal with this in two ways
\subsubsection{Least Square Dummy Variable Method}
For this, we are going to make changes to equation (1). Define a new variable $D_{ki}$ as follows
\[
D_{ki} = \begin{cases} 1 & \text{If $i=k$} \\
                                     0 & \text{Otherwise } \end{cases} , \ k\in\{1,2,...,N\}
\] 
This variable takes 1 if $i$ is the $k$th entity. We are defining this for all $N$ entities. Since we are going to include $\beta_0$, a common intercept, in our regression we need to remove one of the $N$ $D_{ki}$ variables out to avoid dummy variable trap. Let's remove $D_{1i}$ from the equation and include all others, then equation (1) becomes
\[
Y_{it} = \beta_0 +\beta_1X_{it}+\delta_2D_{2i} + ... + \delta_ND_{Ni}+u_{it} \ \tag{LSDV}
\]
To see what this equation gives, we analyze what the intercept will be for each of the $N$ entities. This can be written out as
\[
\begin{aligned}
i=1 \implies & (1): \beta_0 + \beta_2Z_1 &(2): \alpha_1 \ & (3): \beta_0\\ 
i=2 \implies & (1): \beta_0 + \beta_2Z_2 &(2): \alpha_2 \ & (3): \beta_0+\delta_2\\ 
...&...&...&...\\
i=N \implies & (1): \beta_0 + \beta_2Z_N &(2): \alpha_N \ & (3): \beta_0+\delta_N\\ 
\end{aligned}
\]
In all of these cases, slope coefficient in front of $X_{it}$ remains the same. In addition, we control for unobserved cross section fixed effect by allowing the intercept to differ by each $i$. \par\medskip
One clear disadvantage of this method is the computational burden when $N$ or $T$ is large. If $N$ is extremely large, this cannot be solved on pen(cil) and paper. It may even take computer quite a lot of time to process the results. The computational burden cam be eased with the following method. \par\medskip
\subsubsection{Within Transformation Method}
Go back to equation (2). Since we are concerned about cross section fixed effects, we define a new term, $\bar{X}_i, \bar{Y}_i$ as sample mean of $X_{it}, Y_{it}$ for some given $i$ over all possible $t$'s. This can be calculated as
\[
\bar{X}_i = \frac{1}{T}\sum_{t=1}^TX_{it}
\]
Consequently, $\bar{Y}_i$ can be written as
\[
\bar{Y}_i = \frac{1}{T}\sum_{t=1}^TY_{it}=\frac{1}{T}\sum_{t=1}^T\left(\beta_1 X_{it} +\alpha_i +u_{it}\right)=\beta_1 \bar{X}_i +\alpha_i + \bar{u}_{i}
\]
where $\bar{u}_i$ is defined in a similar manner to $\bar{X}_i, \bar{Y}_i$. Then, we subtract $Y_{it}$ by $\bar{Y}_i$ to get
\[
\begin{aligned}
Y_{it}-\bar{Y}_i &= \beta_1(X_{it}-\bar{X}_i) + (u_{it}-\bar{u}_i)\\
\implies \tilde{Y}_{it}&= \beta_1 \tilde{X}_{it}+\tilde{u}_{it}\\
\end{aligned}
\]
where $\tilde{X}_{it}= (X_{it}-\bar{X}_i)$. This controls the fixed effect term by effectively getting rid of them through this transformation process (also known as within transformation). To get the $\beta_1$ estimate, we solve the minimization problem similar to what we have went through in finding the original OLS estimator. This gets us
\[
\hat{\beta}_1= \frac{\sum_{i=1}^n \sum_{t=1}^T \tilde{X}_{it}\tilde{Y}_{it}}{\sum_{i=1}^n \sum_{t=1}^T \tilde{X}_{it}^2}
\]
\begin{mdframed}[backgroundcolor =blue!10]
\textbf{Within estimator}: Again, the minimization problem is to find the $\hat{\beta}_1$ that minimizes the sum of the squared residual terms. This is written as
\[
\min_{\hat{\beta}_1} \sum_{i=1}^N \sum_{t=1}^T (\tilde{Y}_{it}-\hat{\beta}_1 \tilde{X}_{it})^2
\]
Differentiate this term with respect to $\hat{\beta}_1$ to get this first order condition
\[
-2\sum_{i=1}^N \sum_{t=1}^T (\tilde{Y}_{it}-\hat{\beta}_1 \tilde{X}_{it})\tilde{X}_{it}=0
\]
Solving this, we can get $\hat{\beta}_1= \frac{\sum_{i=1}^n \sum_{t=1}^T \tilde{X}_{it}\tilde{Y}_{it}}{\sum_{i=1}^n \sum_{t=1}^T \tilde{X}_{it}^2}$. 
\end{mdframed} \par\medskip
For time fixed effects, all of our logic remains similar. The exception is that we now need to use $\bar{Y}_t$, defined as
\[
\bar{Y}_t = \frac{1}{N}\sum_{i=1}^N(Y_{it}) = \beta_1 \bar{X}_t +\alpha_t +\bar{u}_t
\]
\subsubsection{Time and Entity Fixed Effects}
This is sometimes called a \textbf{two-way fixed effect models}. This is written by
\[
Y_{it}=\beta_1 X_{it} + \alpha_i +\lambda_t + u_{it}
\]
As with the previous fixed effect models, there are two ways to deal with this
\begin{itemize}
\item \textbf{Least square dummy variable}: Now, we include $N-1$ and $T-1$ binary independent variables, along with a common intercept $\beta_0$ to get
\[
Y_{it} = \beta_0 +\beta_1X_{it}+\gamma_2T_{2t}+...+\gamma_TT_{Tt}+\delta_2D_{2i} + ... + \delta_ND_{Ni}+u_{it} 
\]
\item \textbf{Within estimator}: This is a bit complicated, but possible. When both time and entity fixed effects are present, the way of demeaning the data is different. We first demand by time and entity, and then add back the overall sample mean back. In other words, the demeaning process is 
\[
Y_{it}-\bar{Y}_i -\bar{Y}_t +\bar{Y}
\]
where $\bar{Y} = \frac{1}{NT}\sum_{i=1}^N \sum_{t=1}^TY_{it}$ and 
\begin{itemize}
\item $\bar{Y}_i = \beta_1 \bar{X}_i +\alpha_i +\frac{1}{T}\sum_{t=1}^T\lambda_t+\bar{u}_i$
\item $\bar{Y}_t = \beta_1 \bar{X}_t +\frac{1}{N}\sum_{i=1}^N\alpha_i +\lambda_t+\bar{u}_t$
\item $\bar{Y}  =\beta_1\bar{X}+\frac{1}{N}\sum_{i=1}^N\alpha_i+\frac{1}{T}\sum_{t=1}^T\lambda_t+\bar{u}$
\end{itemize}
Then, we get
\[
(Y_{it}-\bar{Y}_i -\bar{Y}_t +\bar{Y}) = \beta_1(X_{it}-\bar{X}_i -\bar{X}_t +\bar{X}) + (u_{it}-\bar{u}_i -\bar{u}_t +\bar{u})
\]
We conduct the OLS estimation here to get the estimate for $\beta_1$. 
\end{itemize}
\subsubsection{Least Square Assumption for Panel Data}
The following assumptions are extensions from what we had in the cross sectional data. However, there are some key differences that are observed in the panel data setting. The four main assumptions are
\begin{itemize}
\item [\textbf{P1}]: $E[u_{it}|X_{i1},..,X_{iT},\alpha_i]=0$. It means that the conditional mean of the $u_{it}$ term does not depend on any of the $X_{it}$ values for entity $i$, whether in the future or in the past. This rules out omitted variable bias. Some books refer to this as strict exogeneity. 
\item [\textbf{P2}]: $(X_{i1},..,X_{iT},u_{i1},...u_{iT})$ is IID across $i=1,..,n$. In other words, variables for one entity are distributed identically to but independent of other entities. However, \textbf{this does not rule out the correlation between $u_{it},u_{ij}$ within entity $i$ for different $j$ and $t$}. Therefore, within an entity, there could be an autocorrelation (or serial correlation).  
\item [\textbf{P3}]: $(X_{it},u_{it})$ have nonzero finite fourth moments. In other words, outliers are very unlikely. We need this condition to derive the asymptotic distribution of the estimators in the panel regression. (Not a scope of this class)
\item [\textbf{P4}]: There is no perfect multicollinearity
\end{itemize}
Because of \textbf{P2}, the standard errors should be calculated while taking into account the possibility of autocorrelation within an entity. This is where \textbf{clustered standard error} comes in. This type of standard errors allow for heteroskedasticity and autocorrelation within an entity. However, it assumes that regression errors are independent across different entities. In practice, clustered standard errors are larger than regular standard errors (but not always). As in the case of heteroskedasticity, we should care about clustering as using ``wrong" standard errors would lead us to making a wrong judgement about the hypothesis we set on some variables. \par\medskip

\end{document}

