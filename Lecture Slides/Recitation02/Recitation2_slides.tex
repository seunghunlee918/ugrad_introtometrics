\documentclass[compress]{beamer}

\mode<presentation>
\usetheme{Madrid}
\definecolor{Columbia}{RGB}{185,217,235}
\definecolor{Columbia2}{RGB}{0,51,160}
\definecolor{Columbia3}{RGB}{0,114,206}
\setbeamercolor{title}{fg=Columbia2}
\setbeamercolor{frametitle}{fg=Columbia2}
\setbeamercolor{block title}{bg=Columbia, fg=Columbia2}
%\setbeamercolor{block body}{fg=Columbia2}
\setbeamercolor{structure}{fg=Columbia}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Columbia2}
\setbeamercolor{subitem}{fg=Columbia2}
\setbeamercolor{section in toc}{fg=Columbia2}
\setbeamercolor{description item}{fg=Columbia}
\setbeamercolor{caption name}{fg=Columbia2}
\setbeamercolor{button}{fg=Columbia2}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage[scaled=0.92]{helvet}
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
%\hypersetup{
%colorlinks=true,
%linkcolor=black,
%filecolor=green, 
%urlcolor=blue,
%}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=Columbia2, fg=white}
\setbeamercolor{title in head/foot}{bg=Columbia3, fg=white}
\setbeamercolor{date in head/foot}{bg=Columbia, fg=Columbia2}
\setbeamercolor{section in head/foot}{bg=Columbia, fg=Columbia2}
\setbeamercolor{headline}{bg=Columbia}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
        \end{beamercolorbox}}%
        \vskip0pt%
    }
\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeamertemplate{headline}{%
  %\begin{beamercolorbox}[ht=5.5ex]{section in head/foot}
    %\vskip2pt\insertnavigation{0.33\paperwidth}\vskip2pt
  %\end{beamercolorbox}%
%}


\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 2]{Recitation 2} % Change this regularly
\author[Seung-hun Lee]{Seung-hun Lee}
\institute[Columbia University]{Columbia University}

\date[]{}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%%%%%%%%%%%%% Section 1. 


% Capture in one slide
% Separate slide for questions

\begin{frame}
\frametitle{Review: Statistical Inference}
\begin{wideitemize}
\item\textbf{Statistical Inference} refers to any process of using data analysis to make guesses on some parameters of a population using a randomly sampled observation from the larger population. 
\item To conduct a statistical inference, one must first identify a statistically testable question (hypothesis), collect and organize the data, carry out an estimation, test the hypothesis, and come to a conclusion using confidence intervals or other methods.
\begin{itemize}
\item \textbf{Estimation}: process of guessing the statistic of interest (sample mean and sample variance). 
\item \textbf{Hypothesis testing}: You test the null hypothesis ($H_0$) is tested against an alternative hypothesis ($H_1$).  
\item \textbf{Confidence interval}: How accurately your statistic of interest is calculated. Typically, researchers use 95\% or 99\% confidence interval.
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference}
\begin{wideitemize}
\item A typical type of question answered through statistical inference
\begin{itemize}
\item Do exposure to radiation during pregnancy affect long-run health and educational outcomes? (Almond et al. 2009, Black et al. 2019) 
\item Do mask regulations reduce Coronavirus infections?
\item Do migrants affect labor market wages in their new societies? Are they different depending on the occupational sector? (Peri et al. 2020)
\end{itemize}
\item The questions are typically composed of two parts
\begin{itemize}
\item A preceding factor ($X$), whether accidental, policy-related, or individual's choice based on will
\item An outcome of interest ($Y$). 
\end{itemize}
\item Once you know your $X$ and $Y$'s you need to collect data on these variables. (Not an easy task in reality)
\end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{Review: Statistical Inference (example)}
\begin{wideitemize}
\item Estimation: Use the data gathered, you derive the sample version of the parameter that you are interested in
\begin{itemize}
\item It could be a sample mean (or differences of the means)
\item In econometrics, you will see $\hat{\beta}$, an estimate of the true $\beta$
\end{itemize}
\item Hypothesis test: You set a hypothesis test to claim something about the parameter of interest
\begin{itemize}
\item Null ($H_0$): Claims about status quo 
\item Alternative ($H_1$): Claim you are trying to make
\end{itemize}
\item Test: To see if estimation from the data supports your claim
\begin{itemize}
\item Compare critical values, construct confidence interval, or use p-value
\end{itemize}
\end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference (example)}
\begin{itemize}
\item Suppose you are interested in the effect of class sizes on test scores. 
\item Define a small classroom is (say, any class with fewer than 20 students) 
\item Then, calculate the sample mean and the sample variance of test scores of each type of classroom. 
\item You now calculate the test statistic: 
\[ 
\frac{\bar{Y}_b-\bar{Y}_s}{\sqrt{\frac{S_b^2}{n_b}+\frac{S_s^2}{n_s}}}
\]
\item Your hypothesis would be "the mean test score of small classroom is different from others". We can write
\[
H_0: E(Y_b)-E(Y_s) = 0\ \text{vs.}\ H_1:E(Y_b)-E(Y_s) \neq 0
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference (example)}
\begin{itemize}
\item To carry out the test, we can do as follows
\begin{itemize}
\item Calculate the test statistics and directly compare with the \textbf{critical value} derived from assuming that the null hypothesis distribution is correct
\begin{itemize}
\item If we have a standard normal and use 5\% significance level, we compare the test statistic against the critical value of 1.96
\end{itemize}
\item You get the \textbf{confidence interval} (usually 95\%) to see if this interval includes 0, the value claimed by the null hypothesis. 
\begin{itemize}
\item If the confidence interval includes 0, then null hypothesis cannot be rejected. Otherwise, null hypothesis is rejected.
\end{itemize}
\item Other way is to see the \textbf{p-value}, which is roughly defined as the probability of finding a more extreme result than the observed data.
\begin{itemize}
\item Typically, we want to see if the p-value is less than 0.05 
\item Even better if less than 0.01
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Review: Statistical Inference Properties}
Desirable properties
\begin{itemize}
\item \textbf{Unbiasedness:} $E(\bar{Y})=\mu_y$, where $\mu_y$ is the true parameter value.
\item \textbf{Efficiency:} $\bar{Y}$ is the efficient estimator if compared against any other estimator $\hat{Y}$, it is the case that $var(\bar{Y})\leq var(\hat{Y})$
\item \textbf{Consistency:} $\bar{Y}$ is consistent if $\bar{Y}$ converges to $\mu_y$ in probability.
\item \textbf{Asymptotic Normality:} The estimator is asymptotically normal if it becomes normally distributed as the number of observation increases (central limit theorem) 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares}
Setup
\begin{itemize}
\item Suppose that the \textbf{population linear regression model} (also known as data generating process in some books) is
\[
Y_i = \beta_0 + \beta_1X_i + \epsilon_i
\]
\item However, we do not know the true values of the population parameters - $\beta_0$ and $\beta_1$
\item An alternative way to approach the problem is to use the \textbf{sample linear regression model} (or just model)
\[
Y_i = \hat{\beta}_0 +\hat{\beta}_1X_i +u_i
\]
where $\hat{\beta}_0, \hat{\beta}_1$ are estimates of ${\beta}_0, {\beta}_1$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares}
Definition
\begin{itemize}
\item The ideal estimator minimizes the squared sum of residuals. 
\item Mathematically, this can be obtained by solving the following minimization problem and the first order conditions
\footnotesize{\begin{gather*}
\min_{\hat{\beta}_0, \hat{\beta}_1} \sum_{i=1}^n (Y_i-\hat{\beta}_0 - \hat{\beta}_1X_i)^2\\
[\hat{\beta}_0]: -2\sum_{i=1}^n(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\\
[\hat{\beta}_1]: -2\sum_{i=1}^nX_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0 
\end{gather*}}\normalsize
The resulting \textbf{least squares estimators} are
\[
\hat{\beta}_0 = \bar{Y}-\hat{\beta}_1\bar{X}, \ \ \hat{\beta}_1=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(X_i-\bar{X})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares}
Assumptions
\begin{itemize}
\item For OLS to be unbiased, consistent, efficient, and asymptotic normal, the following assumptions must be made
\begin{block}{Assumptions}
\begin{itemize}
\item[\textbf{A1}] Linearity: The regression is assumed to be linear in parameters.
\[
\text{Okay: } Y_i = \beta_0 + \beta_1X_i + \beta_2X_i^2+u_i 
\]
\[
\text{Not: } Y_i = \beta_0 + \beta_1X_i + \beta_2^2X_i+u_i,
\]
\item[\textbf{A2}] i.i.d.: $(X_i,Y_i)$ is assumed to be from independent, identical distribution
\item[\textbf{A3}] $E(u_i|X_i)=0$: This means that conditional on letting $X_i$ take a certain value, we are not making any systematical error in the linear regression. This is required for the OLS to be unbiased. 
\item[\textbf{A4}] Homoskedasticity: $var(u_i)=\sigma_2$, or variance of $u_i$ does not depend on $X_i$. If this condition is broken, there exists a \textit{heteroskedasticity}
\end{itemize}
\end{block}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares}
Assumptions
\begin{block}{Assumptions (Continued)}
\begin{itemize}
\item[\textbf{A5}] No Autocorrelation (Serial Correlation): For $i\neq j$, $cov(u_i,u_j)=0$. In other words, error at the previous period does not have any impact on the current period. This is usually broken in time series settings, where the error in the previous period carries over to the next period.
\item[\textbf{A6}] Orthogonality: $cov(X_i,u_i)=0$. Or, the error term and the independent variable is uncorrelated. If otherwise, there exists an \textit{endogeneity}.
\item[\textbf{A7}] No Outliers: Outlier has no impact on the regression results.
\item[\textbf{A8}] $n>K$: There should be more observations than independent variables.
\item[\textbf{A9}] Variability in $X$: Independent variables should take somewhat different values for each observation. Otherwise, multicollinearity problem exists.
\item[\textbf{A10}] Correct Specification: The model has all the necessary independent variables. (Otherwise, omitted variable or too much variable)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares}
Measure of fitness
\begin{itemize}
\item These numbers tell us how informative the sample linear regression we used is in telling us about the population data
\item $\mathbf{R^2}$: It is defined as a fraction of total variation which is explained by the model. Mathematically, this is
\footnotesize{%
\begin{gather*} 
Y_i = \underbrace{\hat{\beta}_0 + \hat{\beta}_1X_i}_{\hat{Y}_i} + u_i, \ \bar{Y} = \underbrace{\hat{\beta}_0 + \hat{\beta}_1\bar{X}}_{\bar{\hat{Y}}} + \bar{u}, \\
\implies Y_i-\bar{Y} = (\hat{Y}_i - \bar{\hat{Y}}) - (u_i - \bar{u}) \\
\implies \sum_{i=1}^n (Y_i-\bar{Y})^2= \sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2+\sum_{i=1}^n(u_i - \bar{u})^2 - 2\sum_{i=1}^n(\hat{Y}_i - \bar{\hat{Y}}) (u_i - \bar{u})
\end{gather*}}\normalsize
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares}
Measure of fitness
\begin{itemize}
\item Note that 
\footnotesize{\[
\sum_{i=1}^n(\hat{Y}_i - \bar{\hat{Y}}) (u_i - \bar{u})=\sum_{i=1}^n\hat{Y}_i{u}_i-\bar{\hat{Y}}\sum_{i=1}^nu_i -\bar{u}\sum_{i=1}^n\hat{Y}_i +n\bar{u}\bar{\hat{Y}}
\]}\normalsize
\item Since the mean of error $u_i$ is assumed to be 0, $\bar{u}=0$ and $\sum_{i=1}^nu_i = n\bar{u}$, we only need to take care of $\sum_{i=1}^n\hat{Y}_i{u}_i$. 
\item Note that it is equal to $\hat{\beta}_0\sum_{i=1}^nu_i + \hat{\beta}_1\sum_{i=1}^nu_iX_i$ (Try replaceing $\hat{Y}_i$)
\item The fact that mean of $u_i$ is 0 and $(X_i, u_i)$ are uncorrelated from A6 makes both term 0. So we are left with
\footnotesize{\begin{gather*}
\underbrace{\sum_{i=1}^n (Y_i-\bar{Y})^2}_{TSS}= \underbrace{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}_{ESS}+\underbrace{\sum_{i=1}^n(u_i - \bar{u})^2}_{RSS} \\
\implies 1=\frac{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} + \frac{\sum_{i=1}^n(u_i - \bar{u})^2 }{\sum_{i=1}^n (Y_i-\bar{Y})^2}
\end{gather*}}\normalsize
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares}
Measure of fitness
\begin{itemize}
\item Thus, the $R^2$ can be found as
\[
R^2 = \frac{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} = \frac{ESS}{TSS} = 1-\frac{RSS}{TSS}
\]
\item Intuitively, higher $R^2$ implies that the model explains more of the total variance, which implies that the regression fits the data well. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares}
Measure of fitness
\begin{itemize}
\item  $\mathbf{SER}$: Standard Error of Regression. It estimate the standard deviation of the error term in $Y_i$, or mathematically
\[
SER = \sqrt{\frac{1}{n-2}\sum_{i=1}^n u_i^2}
\]
where $u_i = y_i-\hat{y}_i$ and we use $n-2$ since there is loss of d.f. by two due to $\hat{\beta}_0, \hat{\beta}_1$. 
\item If SER turns out to be large, this implies that our model might be missing a key variable.
\end{itemize}
\end{frame}
%%%%%%%%%%%
\end{document}
