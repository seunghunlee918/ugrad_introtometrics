\documentclass[compress]{beamer}

\mode<presentation>
\usetheme{Madrid}
\definecolor{Columbia}{RGB}{185,217,235}
\definecolor{Columbia2}{RGB}{0,51,160}
\definecolor{Columbia3}{RGB}{0,114,206}
\setbeamercolor{title}{fg=Columbia2}
\setbeamercolor{frametitle}{fg=Columbia2}
\setbeamercolor{block title}{bg=Columbia, fg=Columbia2}
%\setbeamercolor{block body}{fg=Columbia2}
\setbeamercolor{structure}{fg=Columbia}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Columbia2}
\setbeamercolor{subitem}{fg=Columbia2}
\setbeamercolor{section in toc}{fg=Columbia2}
\setbeamercolor{description item}{fg=Columbia}
\setbeamercolor{caption name}{fg=Columbia2}
\setbeamercolor{button}{fg=Columbia2}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage[scaled=0.92]{helvet}
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
%\hypersetup{
%colorlinks=true,
%linkcolor=black,
%filecolor=green, 
%urlcolor=blue,
%}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=Columbia2, fg=white}
\setbeamercolor{title in head/foot}{bg=Columbia3, fg=white}
\setbeamercolor{date in head/foot}{bg=Columbia, fg=Columbia2}
\setbeamercolor{section in head/foot}{bg=Columbia, fg=Columbia2}
\setbeamercolor{headline}{bg=Columbia}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
        \end{beamercolorbox}}%
        \vskip0pt%
    }
\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeamertemplate{headline}{%
  %\begin{beamercolorbox}[ht=5.5ex]{section in head/foot}
    %\vskip2pt\insertnavigation{0.33\paperwidth}\vskip2pt
  %\end{beamercolorbox}%
%}


\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 8]{Recitation 8} % Change this regularly
\author[Seung-hun Lee]{Seung-hun Lee}
\institute[Columbia University]{Columbia University}

\date[]{}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

%%%%%%%%%%%%% Section 1. 


% Capture in one slide
% Separate slide for questions
\begin{frame}
\frametitle{Binary dependent variables}
Motivation
\begin{itemize}
\item $y$ now  takes either 0 or 1.
\item Can be used to study how independent variable(s) $X_i$ is(are) correlated to yes/no questions in the survey.
\item Assume 
\[
Y_i = \beta_0 + \beta_1 X_i +u_i
\]
\item We look at $E[Y_i|X_i]$, which can be broken into 
\[
E[Y_i|X_i] = 0\times\Pr(Y_i=0|X_i)+1\times\Pr(Y_i=1|X_i)
\]
\item Or in the regression equation context, 
\[
\begin{aligned}
E[Y_i|X_i]&=E[\beta_0+\beta_1X_i+u_i|X_i]\\
&=\beta_0 + \beta_1X_i + E[u_i|X_i]\\
(\because  E[u_i|X_i]=0)&=\beta_0 + \beta_1X_i 
\end{aligned}
\]
or the probability of $Y_i=1$ given $X_i$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Binary dependent variables}
Interpretation
\begin{itemize}
\item Notice that $\beta_1 =\frac{\Delta Y_i}{\Delta X_i}$ and $\Delta Y_i = \text{Change in }\Pr(Y_i=1|X_i)$ with respect to change in $X_i$ , or
\[
\Delta Y_i = \Pr(Y_i=1|X_i=x+\Delta X_i)-\Pr(Y_i=1|X_i=x)
\]
\item Since $\Pr(Y_i=1|X_i=x+\Delta X_i)-\Pr(Y_i=1|X_i=x)=E[Y_i|X_i=x+\Delta X_i]-E[Y_i|X_i=x]$, we get
\[
\beta_0+\beta_1(x+\Delta X_i)-\beta_0+\beta_1(x) =\beta_1 \Delta X_i
\]
\item So we get $\Delta Y_i = \beta_1\Delta X_i\iff\beta_1 =\frac{\Delta Y_i}{\Delta X_i}$. Therefore, $\beta_1$ now measures how much the predicted probability of $Y_i=1$ changes with respect to $X_i$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Binary dependent variables}
Linear probability models
\begin{itemize}
\item \textbf{Linear probability model} is the estimation in which you run an OLS on the type of regression equation where $Y_i$ is a binary dependent variable.
\item The advantage is that it is simple - there is no difference in terms of methods between this and the OLS methods we have learned so far. 
\item However, there are some critical disadvantages to this model. 
\begin{itemize}
\item By setting the regression model as above, we are assuming that the change of predicted probability of $Y_i=1$ is constant for all values of $X_i$. 
\item More critically, it is possible that the predicted probability $\hat{y}$ may be greater than 1 or strictly less than 0.
\item The distribution of the error term is no longer normal distribution, potentially affecting the asymptotic properties of the OLS estimators.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Binary dependent variables}
Logit regression
\begin{itemize}
\item Logit regression: Let $Z_i=\beta_0+\beta_1X_i$. 
\item Logit regression assumes that the cumulative probability of $Z_i$, which is $\Pr(Y_i=1|X_i)$  is distributed as
\[
\Pr(Y_i=1|X_i)=F(Z_i)=\frac{1}{1+e^{-Z_i}}
\]
\item Changes in $X_i$ affect the probability $F(Z_i)$ in this manner
\[
\frac{\partial F}{\partial X_i} = \frac{\partial F}{\partial Z_i}\frac{\partial Z_i}{\partial X_i}  
\]
where $\frac{\partial Z_i}{\partial X_i}  =\beta_1$
\item  Value of $\beta_1$ does not mean that much in. Its sign does, since
\[
 \frac{\partial F}{\partial Z_i}=\frac{e^{-\beta_0 -\beta_1X_i}}{(1+e^{-\beta_0 -\beta_1X_i})^2}
\]
and its sign depends on that of $\beta_1$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Binary dependent variables}
Probit regression
\begin{itemize}
\item Probit regression: Let $Z_i=\beta_0+\beta_1X_i$. 
\item Logit regression assumes that the cumulative probability of $Z_i$ is a standard normal distribution
\[
\Pr(Y_i=1|X_i)=F(Z_i)=\Phi(Z_i)=\Phi(\beta_0+\beta_1X_i)
\]
where $\Phi(v)$ means the cumulative normal function $\Pr(Z\leq v)$
\item Again, taking the similar approach as before, 
\[
\frac{\partial F}{\partial X_i} = \frac{\partial F}{\partial Z_i}\frac{\partial Z_i}{\partial X_i} 
\]
and $\frac{\partial F}{\partial Z_i}$ is the pdf of a standard normal distribution.
\item Again, its sign depends on that of $\beta_1$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimators}
Motivation
\begin{itemize}
\item Both probit and logit are nonlinear: $\beta_0, \beta_1$ parameters are no longer in linear relationship with the $X_i's$ and subsequently $Y_i$'s
\item A \textbf{likelihood function} is the conditional density of $Y_1,...,Y_n$ given $X_1,...,X_n$ that is treated as the function of the unknown parameters ($\beta_0, \beta_1$ in our case)
\item What we are trying to do here is to find the values of $\beta_i$'s that best matches the values of $X_i$'s and $Y_i$'s
\item \textbf{Maximum likelihood estimators} is the value of $\beta_i$'s that best describes the data and maximizes the value of the likelihood function
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimators}
Practice
\begin{itemize}
\item Assume $Y_i$'s are IID normal with mean $\mu$ and standard error $\sigma$ (both are unknown)
\item The joint probability of $Y_i$'s are (our likelihood function)
\[
\begin{aligned}
\Pr(Y_1=y_1,...,Y_n=y_n|\mu,\sigma)&=\Pr(Y_1 = y_1|\mu,\sigma)\times..\times\Pr(Y_n=y_n|\mu,\sigma)\\
&=\prod_{i=1}^nf(y_i|\mu,\sigma)\\
&=\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(Y_i-\mu)^2}{2\sigma^2}}\\
&=(2\pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}}e^{-\sum_{i=1}^n\frac{(Y_i-\mu)^2}{2\sigma^2}}\\
\end{aligned}
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum likelihood estimators}
Practice
\begin{itemize}
\item Calculation is made easier by using log-likelihood functions (take logs to likelihood functions)
\[
-\frac{n}{2}\ln{(2\pi)}-\frac{n}{2}\ln{\sigma^2}-\sum_{i=1}^n\frac{(Y_i-\mu)^2}{2\sigma^2} 
\]
\item We differentiate the above with respect to $\mu$ and $\sigma$ to find the MLE of these parameters.
\item This gets us
\[
\begin{aligned}
\mu_{MLE}&=\frac{1}{n}\sum_{i=1}^nY_i\\
\sigma^2_{MLE}&=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu_{MLE})^2
\end{aligned}
\]
\end{itemize}
\end{frame}
%%%%%%%%%%%
\end{document}
